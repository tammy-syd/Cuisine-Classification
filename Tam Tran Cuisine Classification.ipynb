{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuisine Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I am trying to classify different recipes into the cuisines that they belong to on the basis of the ingredients in the recipe. This dataset has been provided by the Yummly API. This dataset has been taken from the 'What's Cooking' Kaggle competition.\n",
    "The data provided by Kaggle was already split into train and test, with the test set having no labels, so just as other reserachers (Check the end for citations) - Even I have made use of the train set (provided online) as the full set and split that into test and train to measure how good/bad the models are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow of this project - I have a dataset with text input and multiple classes that each of the text pieces need to be classified into. I'm using Natural Language Processing in this Project and then feeding it to different Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re # For pattern matching - Regular Expressions\n",
    "import nltk # For TextProcessing and exploring string distances\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer # Used for Natural Language Processing\n",
    "\n",
    "'''WordNetLemmatizer is used for Parts of Speech Tagging in NLP, it converts a word to its base form. \n",
    "   I learnt more about WordNetLemmatizer and other lemmatization techniques from the below mentioned sources:\n",
    "   https://en.wikipedia.org/wiki/Lemmatisation\n",
    "   https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "   https://www.geeksforgeeks.org/python-lemmatization-with-nltk/'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "'''TF-IDF Vectorization converts textual data to numeric vectors that can be fed to a model.\n",
    "   I learnt more about TFIDF vectorization from the below mentioned sources:\n",
    "   https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.Xri8s2gzZPY\n",
    "   https://monkeylearn.com/blog/beginners-guide-text-vectorization/\n",
    "   https://monkeylearn.com/blog/what-is-tf-idf/\n",
    "   https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html'''\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV # For hyperparameter tuning\n",
    "from sklearn.model_selection import train_test_split # For splitting the data into train and test\n",
    "\n",
    "''' the function train_test_split is also available with sklearn.cross_validation, but usage for that has deprecated'''\n",
    "\n",
    "'''Importing all classification models we will be using'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "'''Importing all metrics for comparing results'''\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Pratiksha\n",
      "[nltk_data]     Sharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The WordNetLemmatizer needs to install first\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset into a dataframe\n",
    "# The dataset was in a json format\n",
    "df=pd.read_json(\"whats-cooking/train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39774 entries, 0 to 39773\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           39774 non-null  int64 \n",
      " 1   cuisine      39774 non-null  object\n",
      " 2   ingredients  39774 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 932.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Knowing more about this dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has no null values, and therefore no preprocessing for cleaning (other than text processing) is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10259</td>\n",
       "      <td>greek</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25693</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20130</td>\n",
       "      <td>filipino</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22213</td>\n",
       "      <td>indian</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13162</td>\n",
       "      <td>indian</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      cuisine                                        ingredients\n",
       "0  10259        greek  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  25693  southern_us  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2  20130     filipino  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3  22213       indian                [water, vegetable oil, wheat, salt]\n",
       "4  13162       indian  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the first couple of rows to understand the format in which the dataset is\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "italian         7838\n",
       "mexican         6438\n",
       "southern_us     4320\n",
       "indian          3003\n",
       "chinese         2673\n",
       "french          2646\n",
       "cajun_creole    1546\n",
       "thai            1539\n",
       "japanese        1423\n",
       "greek           1175\n",
       "spanish          989\n",
       "korean           830\n",
       "vietnamese       825\n",
       "moroccan         821\n",
       "british          804\n",
       "filipino         755\n",
       "irish            667\n",
       "jamaican         526\n",
       "russian          489\n",
       "brazilian        467\n",
       "Name: cuisine, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking what all different cuisine are there in the dataset\n",
    "df[\"cuisine\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are a total of 20 cuisines or 20 classes that a recipe can be classified into.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Next we'll move on to NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with lemmatization of the words in our list of ingredients; i.e. bringing them into their base form - this is also called 'Parts of Speech' tagging. Check above for links to detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We use strip here to trim any leading or trailing white spaces from our inputs after POS tagging and transformation.\n",
    "   ' '.join is used for joining all transformed lemmatized words into a single sentence or input string'''\n",
    "\n",
    "df['ingredients_afterPOStagging']=[' '.join([WordNetLemmatizer().lemmatize(re.sub('[^a-zA-Z]',' ',element)\n",
    "                                                        ) for element in ing]).strip() for ing in df['ingredients']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black pepper', 'shallots', 'cornflour', 'cayenne pepper', 'onions', 'garlic paste', 'milk', 'butter', 'salt', 'lemon juice', 'water', 'chili powder', 'passata', 'oil', 'ground cumin', 'boneless chicken skinless thigh', 'garam masala', 'double cream', 'natural yogurt', 'bay leaf']\n",
      "black pepper shallot cornflour cayenne pepper onion garlic paste milk butter salt lemon juice water chili powder passata oil ground cumin boneless chicken skinless thigh garam masala double cream natural yogurt bay leaf\n"
     ]
    }
   ],
   "source": [
    "'''Seeing the results of our lemmatization'''\n",
    "\n",
    "print(df['ingredients'][4])\n",
    "print(df['ingredients_afterPOStagging'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see from the example above that the word 'shallots' got transformed to 'shallot', 'onions' to 'onion' etcettera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll seperate our inputs from our class labels\n",
    "x=df['ingredients_afterPOStagging']\n",
    "y=df['cuisine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test in the ratio 7:3\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=24,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to transform our data in a way that can be fed to the machine. We use Term Frequency - Inverse Document Frequency for vectorization of our text. We create a TFIDF model that learns its vocabulary from our set of ingredient strings (or 'Documents') and rates the uniqueness or significance of a word on the basis of the number of times it appears in a document and the number of documents it is present in. **By documents I mean the ingredient strings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The parameter stop_words='english' below has an inbuilt list of all pronouns that can confuse our model \n",
    "   if they are present, so as to avoid this, TfidfVectorizer keeps in mind to not add words like these to its\n",
    "   vocabulary'''\n",
    "'''The parameter max_df specifies to what extent a word's presence, throughout all documents, can be tolerated'''\n",
    "\n",
    "vectorizer=TfidfVectorizer(stop_words='english',ngram_range = ( 1 , 1 ),analyzer=\"word\", max_df = .55, token_pattern=r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abalone', 'absinthe', 'abura', 'acai', 'accent', 'accompaniment', 'achiote', 'acid', 'acini', 'ackee', 'acorn', 'acting', 'active', 'added', 'adobo', 'adzuki', 'agar', 'agave', 'age', 'aged', 'ahi', 'ai', 'aioli', 'ajinomoto', 'ajwain', 'aka', 'alaskan', 'albacore', 'alcohol', 'ale', 'aleppo', 'alexia', 'alfalfa', 'alfredo', 'allspice', 'almond', 'almondmilk', 'almonds', 'aloe', 'alum', 'amaranth', 'amarena', 'amaretti', 'amaretto', 'amba', 'amber', 'amberjack', 'amchur', 'american', 'aminos', 'ammonium', 'amontillado', 'ampalaya', 'anaheim', 'anasazi', 'ancho', 'anchovies', 'anchovy', 'andouille', 'anejo', 'angel', 'anglaise', 'angled', 'angostura', 'anise', 'anisette', 'anjou', 'annatto', 'ao', 'aonori', 'apple', 'apples', 'applesauce', 'applewood', 'apricot', 'arak', 'arame', 'arbol', 'arborio', 'arepa', 'argo', 'arhar', 'armagnac', 'arrabbiata', 'arrowroot', 'artichok', 'artichoke', 'artichokes', 'artisan', 'arugula', 'asada', 'asadero', 'asafetida', 'asafoetida', 'asiago', 'asian', 'asparagus', 'aspic', 'assorted', 'asti', 'atar', 'atlantic', 'atta', 'au', 'avocado', 'avocados', 'awase', 'azteca', 'azuki', 'b', 'baby', 'bacardi', 'bacon', 'bag', 'bagel', 'bags', 'baguette', 'bai', 'baileys', 'baked', 'baking', 'balance', 'balls', 'balm', 'balsamic', 'balsamico', 'bamboo', 'banana', 'bananas', 'banger', 'banh', 'baobab', 'bar', 'barbecue', 'barbecued', 'barberries', 'barilla', 'bark', 'barley', 'barolo', 'barramundi', 'bars', 'bartlett', 'basa', 'base', 'based', 'basil', 'basmati', 'bass', 'baton', 'batter', 'bawang', 'bay', 'bayonne', 'bbq', 'bean', 'beans', 'beansprouts', 'bear', 'beaten', 'beau', 'beaujolais', 'bechamel', 'bee', 'beef', 'beefsteak', 'beer', 'beet', 'beetroot', 'beets', 'belacan', 'belgian', 'believ', 'believe', 'bell', 'bellpepper', 'bells', 'belly', 'beluga', 'ben', 'bengal', 'bengali', 'beni', 'benne', 'bens', 'bermuda', 'berries', 'berry', 'bertolli', 'besan', 'best', 'better', 'beurre', 'beverage', 'beverages', 'bhaji', 'bhindi', 'bianco', 'bibb', 'bicarbonate', 'big', 'bihon', 'bing', 'bird', 'biryani', 'biscuit', 'biscuits', 'bison', 'bisquick', 'bits', 'bitter', 'bitters', 'bittersweet', 'black', 'blackberries', 'blackberry', 'blackening', 'blackstrap', 'blade', 'blanc', 'blanched', 'blanco', 'blend', 'blends', 'bliss', 'block', 'blood', 'bloody', 'blossom', 'blossoms', 'blue', 'blueberri', 'blueberries', 'blueberry', 'boar', 'bocconcini', 'boil', 'boiled', 'boiling', 'bok', 'bolillo', 'bologna', 'bone', 'boned', 'boneless', 'bones', 'bonito', 'bonnet', 'borage', 'bordelaise', 'borlotti', 'bosc', 'boston', 'bottle', 'bottled', 'bottoms', 'boudin', 'bought', 'bouillon', 'bouquet', 'bourbon', 'bow', 'boy', 'boysenberry', 'braeburn', 'bragg', 'braggs', 'braising', 'bran', 'branca', 'brand', 'brandy', 'branston', 'branzino', 'brat', 'brats', 'brazil', 'bread', 'breadcrumb', 'breadcrumbs', 'breadstick', 'breakfast', 'breakstone', 'bream', 'breast', 'breasts', 'bresaola', 'brewed', 'breyers', 'brie', 'brien', 'brine', 'brioche', 'bris', 'brisket', 'broad', 'broccoli', 'broccolini', 'broil', 'broiler', 'broth', 'brown', 'brownie', 'browning', 'browns', 'brussels', 'bucatini', 'buckwheat', 'buds', 'budweiser', 'buffalo', 'bulb', 'bulgur', 'bulk', 'bun', 'buns', 'burdock', 'burger', 'burgundi', 'burgundy', 'burrata', 'burro', 'bushi', 'butt', 'butter', 'buttercream', 'buttercup', 'butterflied', 'buttermilk', 'butternut', 'butterscotch', 'buttery', 'button', 'cabbage', 'cabernet', 'cabrales', 'cacao', 'cachaca', 'caciocavallo', 'cactus', 'caesar', 'cai', 'cajeta', 'cajun', 'cake', 'cakes', 'calabash', 'calabaza', 'calabrese', 'calamansi', 'calamari', 'calamata', 'calcium', 'calf', 'california', 'calimyrna', 'callaloo', 'calorie', 'calvados', 'camellia', 'camembert', 'campanelle', 'campari', 'campbell', 'canadian', 'candied', 'candlenut', 'candy', 'cane', 'canela', 'canned', 'cannellini', 'cannelloni', 'canning', 'cannoli', 'canola', 'cantal', 'cantaloupe', 'canton', 'capellini', 'caper', 'capers', 'capicola', 'capocollo', 'capon', 'caps', 'capsicum', 'cara', 'carambola', 'caramel', 'caraway', 'carbonated', 'carcass', 'cardamom', 'cardamon', 'cardoon', 'caribbean', 'carnaroli', 'carnation', 'carne', 'carnitas', 'carp', 'carpaccio', 'carrot', 'carrots', 'casa', 'cascabel', 'casera', 'cashew', 'cashews', 'casing', 'casings', 'cassava', 'cassia', 'cassis', 'castellane', 'castelvetrano', 'caster', 'catalina', 'catfish', 'catsup', 'caul', 'cauliflower', 'cauliflowerets', 'cava', 'cavatappi', 'cavatelli', 'cavenders', 'caviar', 'cavolo', 'cayenne', 'ceci', 'celery', 'celtic', 'center', 'century', 'cereal', 'ch', 'chaat', 'chachere', 'chai', 'challa', 'challenge', 'cham', 'chambord', 'champagne', 'chana', 'chang', 'channa', 'chanterelle', 'chapati', 'char', 'chard', 'chardonnay', 'chartreuse', 'chat', 'chateaubriand', 'chayotes', 'che', 'cheddar', 'chee', 'cheeks', 'chees', 'cheese', 'cheesi', 'chenpi', 'cheong', 'cherries', 'cherry', 'cherrystone', 'chervil', 'chestnut', 'chestnuts', 'chevre', 'chia', 'chianti', 'chicharron', 'chicken', 'chickens', 'chickpea', 'chickpeas', 'chicory', 'chiffonade', 'chihuahua', 'chile', 'chilean', 'chilegarlic', 'chiles', 'chili', 'chilies', 'chilled', 'chilli', 'chiltep', 'chinese', 'chinkiang', 'chioggia', 'chip', 'chipotl', 'chipotle', 'chipotles', 'chipped', 'chips', 'chitterlings', 'chive', 'chives', 'cho', 'chocolate', 'chocolatecovered', 'choi', 'cholesterol', 'cholula', 'chong', 'chop', 'chopmeat', 'chopped', 'chops', 'chorizo', 'chourico', 'chow', 'choy', 'chrysanthemum', 'chua', 'chuck', 'chuka', 'chunk', 'chunks', 'chunky', 'chuno', 'chutney', 'ciabatta', 'cider', 'cilantro', 'cinnamon', 'cipollini', 'citric', 'citron', 'citrus', 'clam', 'clamato', 'clams', 'clarified', 'classic', 'classico', 'claw', 'claws', 'clear', 'clementine', 'clotted', 'cloud', 'clove', 'clover', 'cloves', 'club', 'coars', 'coarse', 'cob', 'coca', 'cockle', 'cocktail', 'coco', 'cocoa', 'coconut', 'cod', 'codfish', 'coffee', 'cognac', 'cointreau', 'coke', 'cola', 'colby', 'cold', 'coleslaw', 'collard', 'collect', 'colman', 'color', 'coloring', 'colouring', 'comfort', 'comino', 'compote', 'compressed', 'concentrate', 'conch', 'condensed', 'condiment', 'confectioners', 'confit', 'conger', 'conimex', 'consomme', 'convert', 'converted', 'cook', 'cooked', 'cooki', 'cookie', 'cookies', 'cooking', 'cooky', 'cool', 'cordial', 'coriander', 'corkscrew', 'corn', 'cornbread', 'corned', 'cornflake', 'cornflakes', 'cornflour', 'cornhusks', 'cornichons', 'cornish', 'cornmeal', 'cornstarch', 'cotija', 'cottage', 'cotto', 'coulis', 'country', 'couscous', 'covered', 'cr', 'crab', 'crabapple', 'crabmeat', 'crabs', 'cracked', 'cracker', 'crackers', 'cranberries', 'cranberry', 'crawfish', 'crayfish', 'cream', 'creamed', 'creamer', 'creami', 'creamy', 'crema', 'creme', 'cremini', 'creole', 'crepe', 'crescent', 'cress', 'crimini', 'crisco', 'crisp', 'crispy', 'crock', 'croissant', 'crookneck', 'cross', 'crostini', 'crouton', 'croutons', 'crumb', 'crumbled', 'crumbles', 'crumbs', 'crunch', 'crush', 'crushed', 'crust', 'crusts', 'crusty', 'crystal', 'crystallized', 'cuban', 'cube', 'cubed', 'cubes', 'cucumber', 'cucumbers', 'cuervo', 'cuisine', 'culantro', 'culinary', 'cultured', 'cumberland', 'cumin', 'cuminseed', 'cummin', 'cura', 'curd', 'curds', 'cured', 'curing', 'curls', 'curly', 'currant', 'currants', 'curry', 'custard', 'cut', 'cutlet', 'cutlets', 'cuts', 'cuttlefish', 'd', 'daal', 'daikon', 'dairy', 'daisy', 'daiya', 'dal', 'dandelion', 'dangmyun', 'daniels', 'darjeeling', 'dark', 'dash', 'dashi', 'date', 'dates', 'day', 'dean', 'decorating', 'deep', 'delallo', 'deli', 'delicata', 'delicious', 'demerara', 'demi', 'dende', 'dessert', 'devein', 'deveined', 'devil', 'dew', 'dhal', 'dhaniya', 'di', 'diamond', 'diced', 'dickel', 'dictine', 'diet', 'digestive', 'dijon', 'dijonnaise', 'dill', 'dillweed', 'dinner', 'dip', 'dipping', 'dips', 'disco', 'dish', 'distilled', 'ditalini', 'doenzang', 'dog', 'dogs', 'dolce', 'dole', 'domino', 'doritos', 'doubanjiang', 'doubl', 'double', 'dough', 'doughs', 'dr', 'dragon', 'drain', 'drained', 'drambuie', 'dress', 'dressing', 'dri', 'dried', 'drink', 'dripping', 'drippings', 'drum', 'drummettes', 'drumstick', 'drumsticks', 'dry', 'du', 'duck', 'duckling', 'dukkah', 'dulce', 'dulong', 'dumpling', 'dungeness', 'durum', 'dusting', 'dutch', 'duxelles', 'e', 'ear', 'earl', 'ears', 'earth', 'eating', 'eau', 'edam', 'edamame', 'edible', 'eel', 'egg', 'eggland', 'eggnog', 'eggplant', 'eggplants', 'eggroll', 'eggs', 'el', 'elbow', 'elderflower', 'elmlea', 'emerils', 'emmenthal', 'empanada', 'enchilada', 'endive', 'energy', 'english', 'enokitake', 'enriched', 'epazote', 'equal', 'erythritol', 'escalopes', 'escargot', 'escarole', 'espelette', 'espresso', 'essence', 'estancia', 'european', 'evaporated', 'everglades', 'extra', 'extract', 'eye', 'eyed', 'fajita', 'falafel', 'family', 'farfalline', 'farina', 'farm', 'farmer', 'farmhouse', 'farms', 'farofa', 'farro', 'fashioned', 'fast', 'fat', 'fatback', 'fatfre', 'fatfree', 'fats', 'fava', 'fed', 'feet', 'fennel', 'fenugreek', 'fermented', 'ferns', 'feta', 'fettuccine', 'fettucine', 'fiber', 'ficelle', 'fiddlehead', 'fideos', 'field', 'fiesta', 'fig', 'figs', 'fil', 'file', 'filet', 'filets', 'filipino', 'filled', 'fillet', 'fillets', 'filling', 'filo', 'fine', 'finely', 'fines', 'finger', 'fingerling', 'fingers', 'fino', 'firm', 'firmly', 'fish', 'fishcake', 'fisher', 'flageolet', 'flaked', 'flakes', 'flan', 'flank', 'flanken', 'flat', 'flatbread', 'flavor', 'flavored', 'flavoring', 'flax', 'flaxseed', 'fleshed', 'fleur', 'flora', 'floret', 'florets', 'flounder', 'flour', 'flower', 'flowerets', 'flowering', 'flowers', 'fluff', 'focaccia', 'foccacia', 'foie', 'fondant', 'fontina', 'food', 'foot', 'forest', 'foster', 'fowl', 'fra', 'framboise', 'frangelico', 'frangipane', 'frank', 'frankfurter', 'franks', 'free', 'freeze', 'fregola', 'french', 'fresca', 'fresco', 'fresh', 'freshly', 'fresno', 'fri', 'fried', 'fries', 'frisee', 'fritos', 'fromage', 'frond', 'fronds', 'frosting', 'frostings', 'frozen', 'fructose', 'fruit', 'fruitcake', 'fruits', 'fry', 'fryer', 'fryers', 'frying', 'fu', 'fudge', 'fuji', 'fully', 'fume', 'fungus', 'furikake', 'fusilli', 'fuyu', 'gaeta', 'gai', 'gala', 'galanga', 'galangal', 'galliano', 'gallo', 'game', 'ganache', 'garam', 'garbanzo', 'garbanzos', 'garden', 'gari', 'garland', 'garlic', 'garni', 'garnish', 'gebhardt', 'gel', 'gelatin', 'gelato', 'gem', 'gember', 'gemelli', 'genoa', 'genoise', 'george', 'germ', 'german', 'ghee', 'gherkin', 'giant', 'giardiniera', 'giblet', 'gin', 'ginger', 'gingerroot', 'gingersnap', 'ginseng', 'gizzards', 'glace', 'glass', 'glaze', 'glazed', 'globe', 'glucose', 'gluten', 'glutinous', 'gnocchi', 'goat', 'gochugaru', 'gochujang', 'godiva', 'goji', 'gold', 'golden', 'goma', 'gomashio', 'good', 'goose', 'gooseberry', 'goreng', 'gorgonzola', 'gouda', 'gourd', 'goya', 'grade', 'graham', 'grain', 'grained', 'gram', 'gran', 'grana', 'granary', 'grand', 'granita', 'granny', 'granola', 'granular', 'granulated', 'granules', 'grape', 'grapefruit', 'grapes', 'grapeseed', 'grappa', 'gras', 'grass', 'grassfed', 'grate', 'grated', 'grating', 'graviera', 'gravlax', 'gravy', 'gray', 'grease', 'great', 'greater', 'greek', 'greekstyl', 'green', 'greens', 'gremolata', 'grenadine', 'grey', 'grigio', 'grill', 'grilled', 'grilling', 'grissini', 'grit', 'grits', 'groats', 'ground', 'groundnut', 'grouper', 'gruy', 'gruyere', 'guacamol', 'guacamole', 'guajillo', 'guanabana', 'guanciale', 'guava', 'guinea', 'guinness', 'gum', 'gumbo', 'guy', 'gyoza', 'haas', 'habanero', 'haddock', 'hair', 'hake', 'half', 'halibut', 'halloumi', 'halves', 'ham', 'hamachi', 'hamburger', 'hand', 'hanger', 'hanh', 'hanout', 'hard', 'haricot', 'haricots', 'harina', 'harissa', 'harvest', 'hash', 'hass', 'hatch', 'hatcho', 'havarti', 'hawaiian', 'hazelnut', 'hazelnuts', 'head', 'heads', 'heart', 'hearts', 'heath', 'heavy', 'heeng', 'heinz', 'heirloom', 'helix', 'hellmann', 'hemp', 'hen', 'hens', 'herb', 'herbed', 'herbes', 'herbs', 'herbsaint', 'herdez', 'hero', 'herring', 'hibiscus', 'hidden', 'high', 'hijiki', 'hillshire', 'himalayan', 'hing', 'hip', 'hoagie', 'hock', 'hocks', 'hog', 'hoi', 'hoisin', 'hoja', 'holland', 'hollandaise', 'holy', 'home', 'homemade', 'homestyl', 'hominy', 'honey', 'honeycomb', 'honeydew', 'honeysuckle', 'hong', 'hoop', 'horseradish', 'hot', 'hothouse', 'house', 'hsing', 'hubbard', 'huckleberry', 'huitlacoche', 'hummus', 'hungarian', 'hurst', 'husks', 'hyssop', 'ibarra', 'ic', 'ice', 'iceberg', 'iced', 'icing', 'idaho', 'idli', 'imitation', 'imo', 'imperial', 'inch', 'india', 'indian', 'instant', 'iodized', 'irish', 'iron', 'island', 'islands', 'italian', 'jack', 'jackfruit', 'jagermeister', 'jaggery', 'jalape', 'jalapeno', 'jalapenos', 'jam', 'jamaica', 'jamaican', 'jambalaya', 'jambon', 'jameson', 'jamon', 'japanese', 'jarlsberg', 'jasmine', 'jeera', 'jell', 'jelli', 'jello', 'jelly', 'jerk', 'jerky', 'jerusalem', 'jicama', 'jif', 'jiffy', 'jimmy', 'johnsonville', 'jonshonville', 'jose', 'jowl', 'juice', 'jujube', 'jujubes', 'jumbo', 'juniper', 'jus', 'kabocha', 'kabuli', 'kaffir', 'kahl', 'kahlua', 'kaiser', 'kalamansi', 'kalamata', 'kale', 'kalonji', 'kamaboko', 'kampyo', 'kangkong', 'kappa', 'karashi', 'karo', 'kasha', 'kasseri', 'kasu', 'kasuri', 'katakuriko', 'katsuo', 'kecap', 'keema', 'kefalotiri', 'kefalotyri', 'kefir', 'kelp', 'kernel', 'kernels', 'kerrygold', 'ketchup', 'kewpie', 'kewra', 'key', 'khoa', 'kidnei', 'kidney', 'kidneys', 'kielbasa', 'kikkoman', 'kim', 'kimchi', 'kinchay', 'king', 'kirby', 'kirsch', 'kirschenliqueur', 'kirschwasser', 'kiwi', 'kiwifruit', 'knockwurst', 'knoflook', 'knorr', 'knox', 'knuckle', 'knudsen', 'kochu', 'kochujang', 'kohlrabi', 'kokum', 'komatsuna', 'kombu', 'konbu', 'kong', 'konnyaku', 'korean', 'korma', 'kosher', 'kraft', 'krispies', 'kumquat', 'kumquats', 'kung', 'la', 'lacinato', 'lady', 'ladyfinger', 'ladys', 'lager', 'lakes', 'laksa', 'lamb', 'lambic', 'lambrusco', 'lambs', 'lan', 'land', 'lap', 'lapsang', 'lard', 'lardons', 'large', 'lasagna', 'lasagne', 'laurel', 'lavash', 'lavender', 'lb', 'lea', 'leaf', 'leafy', 'lean', 'leav', 'leaves', 'leche', 'lecithin', 'leek', 'leeks', 'leftover', 'leg', 'legs', 'legume', 'lemon', 'lemonade', 'lemongrass', 'lentil', 'lentilles', 'lentils', 'lesser', 'lettuc', 'lettuce', 'lettuces', 'levain', 'licor', 'licorice', 'light', 'lillet', 'lily', 'lima', 'lime', 'limeade', 'limoncello', 'lingcod', 'linguica', 'linguine', 'linguini', 'linguisa', 'link', 'links', 'lipton', 'liqueur', 'liquid', 'liquor', 'liquorice', 'lite', 'littleneck', 'liver', 'livers', 'liverwurst', 'lo', 'loaf', 'loaves', 'lobster', 'lobsters', 'loin', 'london', 'long', 'longan', 'longaniza', 'loofah', 'loosely', 'lop', 'lotus', 'louisiana', 'low', 'lower', 'lowfat', 'lowsodium', 'lox', 'luke', 'lump', 'lumpia', 'luncheon', 'lychee', 'm', 'maca', 'macadamia', 'macaroni', 'macarons', 'mace', 'machine', 'mackerel', 'madeira', 'madeleine', 'madras', 'maggi', 'magret', 'mahi', 'mahimahi', 'maida', 'maifun', 'maitake', 'makers', 'maldon', 'malt', 'malted', 'maltose', 'mam', 'manchego', 'mandarin', 'mango', 'mani', 'manicotti', 'manioc', 'manis', 'manischewitz', 'manzanilla', 'maple', 'maraschino', 'marble', 'marcona', 'margarine', 'margarita', 'margherita', 'marin', 'marinade', 'marinara', 'marjoram', 'mark', 'marmalade', 'marmite', 'marnier', 'marrow', 'marsala', 'marshmallow', 'marshmallows', 'mary', 'marzano', 'marzipan', 'masa', 'masago', 'masala', 'mascarpone', 'mashed', 'masoor', 'massaman', 'massey', 'matcha', 'mature', 'matzo', 'maui', 'mayer', 'mayonaise', 'mayonnais', 'mayonnaise', 'mazola', 'mccormick', 'mcintosh', 'meal', 'meat', 'meatball', 'meatballs', 'meatloaf', 'meats', 'medal', 'medallions', 'medium', 'medjool', 'mein', 'melba', 'mellow', 'melon', 'melted', 'membrillo', 'menta', 'menthe', 'mentsuyu', 'merguez', 'meringue', 'merlot', 'merluza', 'mesclun', 'mesquite', 'methi', 'mex', 'mexican', 'mexicana', 'mexico', 'mexicorn', 'meyer', 'mezcal', 'mezzetta', 'mi', 'mian', 'microgreens', 'mie', 'mignon', 'mild', 'milk', 'milkfish', 'millet', 'min', 'mince', 'minced', 'mincemeat', 'mineral', 'mini', 'miniature', 'minicub', 'minicubes', 'mint', 'minute', 'miracle', 'mirin', 'mirlitons', 'miso', 'mission', 'mitsuba', 'mix', 'mixed', 'mixers', 'mixture', 'mizkan', 'mizuna', 'mo', 'mochi', 'mochiko', 'moisture', 'molasses', 'mole', 'mondavi', 'monde', 'monkfish', 'monterey', 'montreal', 'mooli', 'moong', 'moonshine', 'moose', 'mora', 'morcilla', 'morel', 'moroccan', 'morsel', 'morsels', 'mortadella', 'morton', 'moss', 'mostaccioli', 'mostarda', 'mountain', 'mousse', 'mozarella', 'mozzarella', 'mrs', 'ms', 'msg', 'muenster', 'muffin', 'muffins', 'mulato', 'mullet', 'multi', 'mung', 'muscadet', 'muscadine', 'muscat', 'muscovado', 'muscovy', 'mushroom', 'mushrooms', 'mussel', 'mussels', 'mustard', 'mutton', 'myzithra', 'n', 'naan', 'nacho', 'nam', 'nama', 'napa', 'nappa', 'nashi', 'natto', 'natural', 'navel', 'navy', 'neapolitan', 'neck', 'nectar', 'nectarine', 'needles', 'negi', 'negro', 'nero', 'nestle', 'nests', 'nettle', 'neufch', 'neutral', 'new', 'ni', 'nibs', 'nido', 'nielsen', 'nigari', 'nigella', 'nilla', 'noir', 'non', 'nondairy', 'nonfat', 'nonhydrogenated', 'nonstick', 'noodl', 'noodle', 'noodles', 'nopales', 'nopalitos', 'nori', 'northern', 'nuggets', 'nuoc', 'nut', 'nutella', 'nutmeg', 'nutmegs', 'nutritional', 'nuts', 'o', 'oat', 'oatmeal', 'oats', 'ocean', 'octopus', 'oigatsuo', 'oil', 'oise', 'okra', 'old', 'olek', 'oleo', 'olie', 'oliv', 'olive', 'olives', 'oloroso', 'onion', 'onions', 'orang', 'orange', 'oranges', 'orchid', 'orecchiette', 'oregano', 'oreo', 'organic', 'oriental', 'original', 'originals', 'ornamental', 'ortega', 'orzo', 'oscar', 'osetra', 'ounc', 'ouzo', 'oven', 'ox', 'oxtail', 'oyster', 'oysters', 'oz', 'p', 'paccheri', 'pace', 'pack', 'packed', 'pad', 'padano', 'paddles', 'paddy', 'padron', 'paella', 'pak', 'pale', 'palm', 'pam', 'pan', 'panang', 'pancake', 'pancakes', 'pancetta', 'panch', 'pancit', 'pandan', 'pandanus', 'pane', 'paneer', 'panela', 'panetini', 'panettone', 'pangasius', 'panko', 'pansy', 'pao', 'papad', 'papalo', 'papaya', 'paper', 'pappadams', 'pappardelle', 'paprika', 'paratha', 'parboiled', 'pareve', 'parma', 'parmagiano', 'parmesan', 'parmigiana', 'parmigiano', 'parslei', 'parsley', 'parsnip', 'parts', 'pasilla', 'paso', 'passata', 'passion', 'passover', 'pasta', 'paste', 'pastina', 'pastis', 'pastry', 'pate', 'patis', 'patties', 'pattypan', 'pea', 'peach', 'peaches', 'peanut', 'peanuts', 'peapods', 'pear', 'pearl', 'pearled', 'pearls', 'pears', 'peas', 'peasant', 'pecan', 'pecans', 'pecorino', 'pectin', 'peel', 'peeled', 'pekoe', 'penn', 'penne', 'pepe', 'peperoncini', 'peperoncino', 'pepitas', 'peppadews', 'pepper', 'peppercorn', 'peppercorns', 'peppered', 'pepperidge', 'peppermint', 'pepperocini', 'pepperoncini', 'pepperoni', 'peppers', 'perch', 'perciatelli', 'perilla', 'pernod', 'perrins', 'persian', 'persimmon', 'persimmons', 'pesto', 'petals', 'pete', 'petit', 'petite', 'pheasant', 'philadelphia', 'pho', 'phoran', 'phyllo', 'picante', 'piccolini', 'pices', 'picholine', 'pickapeppa', 'pickle', 'pickled', 'pickles', 'pickling', 'pico', 'pie', 'piece', 'pieces', 'piecrust', 'piecrusts', 'pierogi', 'pig', 'pigeon', 'pignolis', 'pike', 'pilaf', 'pillsbury', 'piloncillo', 'pilsner', 'piment', 'pimento', 'pimenton', 'pimentos', 'pine', 'pineapple', 'pineapples', 'pinenuts', 'pinhead', 'pink', 'pinot', 'pinto', 'piquillo', 'piquin', 'piri', 'pisco', 'pistachio', 'pistachios', 'pistou', 'pit', 'pita', 'pitted', 'pizza', 'pla', 'plain', 'plantain', 'plantains', 'plum', 'plus', 'poblano', 'pocket', 'pockets', 'pods', 'poha', 'poi', 'pointed', 'points', 'poire', 'pois', 'polenta', 'polish', 'pollen', 'pollock', 'pomegranate', 'pomelo', 'pompano', 'ponzu', 'pop', 'popcorn', 'poppadoms', 'popped', 'poppy', 'poppyseeds', 'porcini', 'pork', 'porridge', 'port', 'portabello', 'porter', 'porterhouse', 'portobello', 'posole', 'pot', 'potato', 'potatoes', 'pots', 'potsticker', 'poultry', 'pound', 'poundcake', 'poured', 'poussin', 'powder', 'powdered', 'pozole', 'praline', 'prawn', 'prawns', 'prebaked', 'precooked', 'prego', 'premium', 'prepar', 'prepared', 'preserv', 'preserve', 'preserved', 'preserves', 'pressed', 'pretzels', 'prik', 'prime', 'process', 'processed', 'progresso', 'promise', 'prosciutto', 'proscuitto', 'prosecco', 'protein', 'provence', 'provolone', 'prune', 'prunes', 'psyllium', 'pudding', 'puff', 'puffed', 'pullman', 'pulp', 'pummelo', 'pumpernickel', 'pumpkin', 'pumpkinseed', 'pumpkinseeds', 'pur', 'pure', 'puree', 'purple', 'purpos', 'purpose', 'puy', 'qua', 'quahog', 'quail', 'quarters', 'quatre', 'queso', 'quick', 'quickcooking', 'quince', 'quinoa', 'quorn', 'rabbit', 'rabe', 'rack', 'racks', 'raclette', 'radicchio', 'radish', 'radishes', 'ragu', 'rainbow', 'raisin', 'raising', 'raisins', 'raita', 'rajma', 'ramen', 'ramp', 'ranch', 'range', 'rape', 'rapeseed', 'rapid', 'ras', 'rashers', 'raspberri', 'raspberries', 'raspberry', 'ravioli', 'ravva', 'raw', 'razor', 'ready', 'real', 'reblochon', 'recip', 'recipe', 'red', 'redcurrant', 'redfish', 'redhot', 'reduc', 'reduced', 'reduction', 'refried', 'refrigerated', 'reggiano', 'regular', 'relish', 'remoulade', 'rendered', 'rennet', 'rhubarb', 'rib', 'ribeye', 'ribs', 'rice', 'rich', 'ricotta', 'riesling', 'rigate', 'rigatoni', 'rind', 'rings', 'rins', 'rioja', 'ripe', 'ripened', 'rise', 'rising', 'risotto', 'ritz', 'ro', 'road', 'roast', 'roasted', 'roasting', 'robert', 'robiola', 'robusto', 'rock', 'rocket', 'rockfish', 'roe', 'roll', 'rolled', 'rolls', 'roma', 'romain', 'romaine', 'romana', 'romanesco', 'romano', 'rome', 'rooster', 'root', 'roots', 'roquefort', 'ros', 'rosa', 'rose', 'rosemari', 'rosemary', 'rosewater', 'rotel', 'rotelle', 'roti', 'rotini', 'rotisserie', 'rouille', 'round', 'rounds', 'roux', 'royal', 'rub', 'rubbed', 'ruby', 'rum', 'rump', 'runny', 'russet', 'russian', 'rustic', 'rutabaga', 'rye', 's', 'saba', 'sablefish', 'sack', 'safflower', 'saffron', 'sage', 'sago', 'saigon', 'sake', 'salad', 'salami', 'salata', 'salmon', 'salsa', 'salsify', 'salted', 'saltine', 'saltines', 'sambal', 'sambuca', 'samphire', 'san', 'sanding', 'sandwich', 'sangiovese', 'sangria', 'sansho', 'santa', 'santo', 'sardine', 'sardines', 'sargento', 'sashimi', 'sato', 'satsuma', 'sauc', 'sauce', 'sauces', 'sauerkraut', 'sausag', 'sausage', 'sausages', 'sauterne', 'sauvignon', 'savoiardi', 'savory', 'savoy', 'sazon', 'scallion', 'scallop', 'scallopini', 'scallops', 'scape', 'schmaltz', 'schnapps', 'scotch', 'scrod', 'scrub', 'scrubbed', 'sea', 'seafood', 'season', 'seasoned', 'seasoning', 'seasons', 'seaweed', 'sec', 'seca', 'sections', 'seed', 'seedless', 'seeds', 'segments', 'seitan', 'sel', 'self', 'seltzer', 'semi', 'semisweet', 'semolina', 'serrano', 'serving', 'sesame', 'seven', 'seville', 'shahi', 'shallot', 'shallots', 'shanghai', 'shank', 'shanks', 'shao', 'shaoxing', 'shark', 'sharp', 'shaved', 'shavings', 'sheep', 'sheepshead', 'sheets', 'shell', 'shelled', 'shellfish', 'shells', 'sherbet', 'sherry', 'shichimi', 'shiitake', 'shimeji', 'shin', 'shirataki', 'shiraz', 'shiro', 'shiromiso', 'shiso', 'shoepeg', 'shoga', 'shoots', 'short', 'shortbread', 'shortcake', 'shortcrust', 'shortening', 'shoulder', 'shoyu', 'shred', 'shredded', 'shrimp', 'shuck', 'shucked', 'shungiku', 'sichuan', 'sichuanese', 'sicilian', 'sides', 'silken', 'silver', 'simple', 'single', 'sirloin', 'siu', 'size', 'skate', 'skim', 'skimmed', 'skin', 'skinless', 'skinned', 'skins', 'skippy', 'skirt', 'slab', 'slaw', 'slice', 'sliced', 'slices', 'slider', 'slim', 'slivered', 'sloe', 'small', 'smart', 'smith', 'smithfield', 'smoke', 'smoked', 'smooth', 'snail', 'snails', 'snap', 'snapper', 'snip', 'snow', 'soaking', 'soba', 'sockeye', 'soda', 'sodium', 'sofrito', 'soft', 'soften', 'softened', 'soi', 'sole', 'solid', 'soman', 'sooji', 'soppressata', 'sorbet', 'sorghum', 'sorrel', 'souchong', 'soup', 'sour', 'sourdough', 'soursop', 'southern', 'southwest', 'soy', 'soya', 'soybean', 'soymilk', 'spaghetti', 'spaghettini', 'spam', 'spanish', 'spare', 'sparerib', 'spareribs', 'sparkling', 'spearmint', 'spears', 'specials', 'speck', 'spelt', 'spice', 'spiced', 'spices', 'spicy', 'spike', 'spinach', 'spiny', 'spiral', 'splenda', 'split', 'sponge', 'sports', 'spray', 'spread', 'spreadable', 'sprig', 'sprigs', 'spring', 'sprinkle', 'sprinkles', 'sprite', 'sprout', 'sprouts', 'spumante', 'squab', 'squash', 'squeezed', 'squid', 'squirt', 'sriracha', 'star', 'starch', 'starchy', 'starter', 'steak', 'steaks', 'steamed', 'steamer', 'steel', 'stellette', 'stem', 'stems', 'stevia', 'stew', 'stewed', 'stewing', 'stick', 'stickers', 'sticks', 'sticky', 'stilton', 'stir', 'stock', 'stolichnaya', 'stone', 'stonefire', 'store', 'stout', 'straight', 'strained', 'straw', 'strawberri', 'strawberries', 'strawberry', 'streaky', 'string', 'strip', 'striped', 'strips', 'strong', 'strozzapreti', 'stuffed', 'stuffing', 'sturgeon', 'style', 'sub', 'submarine', 'substitute', 'sucanat', 'success', 'suckling', 'sucralose', 'sucrolose', 'suet', 'sugar', 'sugarcane', 'sugars', 'sultana', 'sum', 'sumac', 'summer', 'sun', 'sunchoke', 'sundae', 'sundried', 'sunflower', 'superfine', 'superior', 'sushi', 'swanson', 'swede', 'sweet', 'sweetbread', 'sweeten', 'sweetened', 'sweetener', 'swerve', 'swiss', 'swordfish', 'syd', 'syrup', 'szechwan', 't', 'tabasco', 'table', 'taco', 'tagliarini', 'tagliatelle', 'tahini', 'tail', 'tails', 'taiwanese', 'taleggio', 'tallow', 'tamale', 'tamari', 'tamarind', 'tandoori', 'tangelo', 'tangerine', 'tangzhong', 'tap', 'tapatio', 'tapenade', 'tapioca', 'tarama', 'tarde', 'taro', 'tarragon', 'tart', 'tartar', 'tartlet', 'tasso', 'taste', 'tater', 'tatsoi', 'tawny', 'te', 'tea', 'teardrop', 'teff', 'tel', 'teleme', 'tempeh', 'tempura', 'tender', 'tenderizer', 'tenderloin', 'tenderloins', 'tenders', 'tendons', 'tentacle', 'tequila', 'teriyaki', 'terrine', 'tex', 'texas', 'textured', 'thai', 'thaw', 'thawed', 'thickener', 'thigh', 'thighs', 'thinli', 'thousand', 'thread', 'threads', 'thyme', 'tie', 'tiger', 'tikka', 'tilapia', 'tip', 'tipo', 'tips', 'toast', 'toasted', 'toasts', 'tobiko', 'toffee', 'tofu', 'togarashi', 'tokyo', 'tom', 'tomate', 'tomatillo', 'tomato', 'tomatoes', 'ton', 'tongue', 'tonic', 'tonkatsu', 'tony', 'toor', 'topping', 'toppings', 'tops', 'topside', 'torn', 'torpedo', 'tortellini', 'tortelloni', 'tortilla', 'tortillas', 'tostada', 'tostitos', 'tots', 'touch', 'toulouse', 'tradicional', 'tradit', 'traditional', 'treacle', 'tree', 'tri', 'tricolor', 'trimmed', 'tripe', 'triple', 'triscuits', 'tropic', 'trotters', 'trout', 'true', 'truffle', 'truffles', 'trumpet', 'truv', 'tsuyu', 'tuaca', 'tube', 'tubetti', 'tubettini', 'tumeric', 'tuna', 'tuong', 'turbinado', 'turbot', 'turkei', 'turkey', 'turkish', 'turmeric', 'turnip', 'turnips', 'turtle', 'tuscan', 'tvp', 'twist', 'twists', 'tzatziki', 'udon', 'ulek', 'ume', 'umeboshi', 'unagi', 'unbaked', 'unbleached', 'uncle', 'uncook', 'uncooked', 'undiluted', 'undrain', 'unflavored', 'uni', 'unsalt', 'unsalted', 'unseasoned', 'unsmoked', 'unsulphured', 'unsweeten', 'unsweetened', 'urad', 'usukuchi', 'v', 'val', 'valencia', 'valley', 'vanilla', 'varieti', 'varnish', 'vay', 'veal', 'vegan', 'veget', 'vegeta', 'vegetable', 'vegetables', 'vegetarian', 'veggie', 'velveeta', 'velvet', 'venison', 'vera', 'verbena', 'verde', 'veri', 'verjuice', 'verjus', 'vermicelli', 'vermouth', 'verts', 'viande', 'victoria', 'vidalia', 'vie', 'vietnamese', 'vin', 'vinaigrette', 'vindaloo', 'vine', 'vinegar', 'vineyard', 'virgin', 'virginia', 'vital', 'vitamin', 'vodka', 'vre', 'wafer', 'wafers', 'waffle', 'wagon', 'wakame', 'walnut', 'walnuts', 'warm', 'wasabi', 'water', 'watercress', 'watermelon', 'wax', 'waxy', 'wedge', 'wedges', 'weed', 'wensleydale', 'wesson', 'wheat', 'wheatberries', 'wheel', 'wheels', 'whey', 'whip', 'whipped', 'whipping', 'whiskey', 'whisky', 'white', 'whitefish', 'whites', 'wholemeal', 'wide', 'wild', 'wildflower', 'williams', 'wine', 'winesap', 'wing', 'wings', 'winter', 'wish', 'wok', 'woksaus', 'wolf', 'wolfberries', 'won', 'wondra', 'wonton', 'wood', 'worcestershire', 'world', 'wrappers', 'wraps', 'xanthan', 'yaki', 'yakisoba', 'yam', 'yams', 'yardlong', 'yeast', 'yellow', 'yellowfin', 'yellowtail', 'yoghurt', 'yogurt', 'yolk', 'yolks', 'yoplait', 'york', 'young', 'yu', 'yuca', 'yucca', 'yukon', 'yum', 'yuzu', 'yuzukosho', 'za', 'zatarains', 'zest', 'zesty', 'zinfandel', 'ziti', 'zucchini']\n"
     ]
    }
   ],
   "source": [
    "'''The function fit_transform combines the task of fitting our input into to the TFIDF model and uses it to transform\n",
    "   all input strings into a sparse matrix.'''\n",
    "vectorized_x_train=vectorizer.fit_transform(x_train)\n",
    "\n",
    "'''Since our inputs were already fitted on our model in the last step we do not need to fit again, so we just \n",
    "   transform the representation of our test set into a sparse matrix '''\n",
    "vectorized_x_test=vectorizer.transform(x_test)\n",
    "\n",
    "'''This is how our model's vocabulary looks like'''\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The next step is creation, training and testing of our models:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None, param_grid={'C': [1, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify=GridSearchCV(LogisticRegression(),{'C':[1,10]}) # Creation of model\n",
    "classify.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Training this model\n",
    "classify=classify.fit(vectorized_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic Regression:  0.7836252409285176\n"
     ]
    }
   ],
   "source": [
    "print('Multinomial Logistic Regression: ',classify.score(vectorized_x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We will be creating 300 trees'''\n",
    "\n",
    "classify2= RandomForestClassifier(n_estimators=300) # Creation of model\n",
    "classify2=classify2.fit(vectorized_x_train,y_train) # Training this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of this model\n",
    "predictions2=classify2.predict(vectorized_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n   brazilian       0.81      0.43      0.56       156\\n     british       0.73      0.24      0.36       223\\ncajun_creole       0.82      0.66      0.73       467\\n     chinese       0.72      0.89      0.80       819\\n    filipino       0.84      0.52      0.64       231\\n      french       0.61      0.52      0.56       792\\n       greek       0.87      0.50      0.64       376\\n      indian       0.83      0.92      0.87       898\\n       irish       0.78      0.29      0.42       201\\n     italian       0.69      0.92      0.79      2312\\n    jamaican       0.97      0.53      0.69       143\\n    japanese       0.83      0.64      0.72       421\\n      korean       0.88      0.64      0.74       241\\n     mexican       0.84      0.93      0.88      1969\\n    moroccan       0.91      0.60      0.72       274\\n     russian       0.86      0.21      0.34       147\\n southern_us       0.63      0.77      0.69      1246\\n     spanish       0.84      0.29      0.43       311\\n        thai       0.77      0.76      0.76       452\\n  vietnamese       0.87      0.41      0.56       254\\n\\n    accuracy                           0.75     11933\\n   macro avg       0.80      0.58      0.64     11933\\nweighted avg       0.76      0.75      0.73     11933\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''To measure the performance of this classifier'''\n",
    "classification_report(y_test,predictions2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier 0.7467527025894578\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest Classifier',classify2.score(vectorized_x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We will be creating 300 stumps'''\n",
    "\n",
    "classify3= AdaBoostClassifier(n_estimators=300) # Creation of model\n",
    "classify3=classify3.fit(vectorized_x_train,y_train) # Training this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of this model\n",
    "predictions3=classify3.predict(vectorized_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Classifier with 300 stumps 0.4418000502807341\n"
     ]
    }
   ],
   "source": [
    "print('AdaBoost Classifier with 300 stumps',classify3.score(vectorized_x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a result which is extremely bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify4=SGDClassifier() # Creation of model\n",
    "classify4=classify4.fit(vectorized_x_train,y_train) # Training this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier:  0.7734852928852761\n"
     ]
    }
   ],
   "source": [
    "print('SGD Classifier: ',classify4.score(vectorized_x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LinearSVC Classifier : Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify5=GridSearchCV(LinearSVC(C=0.8,dual=False),{'C':[1,10]}) # creation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify5=classify5.fit(vectorized_x_train,y_train) # Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Classifier:  0.7860554764099555\n"
     ]
    }
   ],
   "source": [
    "print('LinearSVC Classifier: ',classify5.score(vectorized_x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try creating another TFID vectorizer model by aloowing max_df to be 0.7 to see if it improves any of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2=TfidfVectorizer(stop_words='english',ngram_range = ( 1 , 1 ),analyzer=\"word\", max_df = .70 , token_pattern=r'\\w+')\n",
    "secvectorized_x_train=vectorizer2.fit_transform(x_train).todense()\n",
    "secvectorized_x_test=vectorizer2.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "'''Trying with Logistic Regression'''\n",
    "\n",
    "secclassify=GridSearchCV(LogisticRegression(),{'C':[1,10]})\n",
    "secclassify=secclassify.fit(secvectorized_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Multinomial Logistic Regression Classifier:  0.7856364702924663\n"
     ]
    }
   ],
   "source": [
    "print('New Multinomial Logistic Regression Classifier: ',secclassify.score(secvectorized_x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improvement is not significant, but further exploration with different paramenters might yield a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can rank our models this way:**\n",
    "    1. Linear SVC                         : 78.60%\n",
    "    2. New Multinomial Logistic Regression: 78.56%\n",
    "    3. Multinomial Logistic Regression    : 78.36%\n",
    "    3. SGD Classifier                     : 77.34%\n",
    "    4. Random Forest                      : 74.67%  \n",
    "    5. AdaBoost Classifier                : 44.18%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "S. Kalajdziski, G. Radevski, I. Ivanoska, K. Trivodaliev and B. R. Stojkoska, \"Cuisine classification using recipe's ingredients,\" 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), Opatija, 2018, pp. 1074-1079, doi: 10.23919/MIPRO.2018.8400196."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Readings**\n",
    "\n",
    "S. Jayaraman, T. Choudhury and P. Kumar, \"Analysis of classification models based on cuisine prediction using machine learning,\" 2017 International Conference On Smart Technologies For Smart Nation (SmartTechCon), Bangalore, 2017, pp. 1485-1490, doi: 10.1109/SmartTechCon.2017.8358611."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
